{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16395535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from monai.networks.blocks import UnetrBasicBlock, UnetrPrUpBlock, UnetrUpBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from monai.networks.nets import ViT\n",
    "\n",
    "\n",
    "class UNETR(nn.Module):\n",
    "    \"\"\"\n",
    "    UNETR based on: \"Hatamizadeh et al.,\n",
    "    UNETR: Transformers for 3D Medical Image Segmentation <https://arxiv.org/abs/2103.10504>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        img_size: Tuple[int, int, int],\n",
    "        feature_size: int = 16, # - Feature size is 64 in the image\n",
    "        hidden_size: int = 768,\n",
    "        mlp_dim: int = 3072,\n",
    "        num_heads: int = 12,\n",
    "        pos_embed: str = \"perceptron\",\n",
    "        norm_name: Union[Tuple, str] = \"instance\",\n",
    "        conv_block: bool = False,\n",
    "        res_block: bool = True,\n",
    "        dropout_rate: float = 0.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: dimension of input channels.\n",
    "            out_channels: dimension of output channels.\n",
    "            img_size: dimension of input image.\n",
    "            feature_size: dimension of network feature size.\n",
    "            hidden_size: dimension of hidden layer.\n",
    "            mlp_dim: dimension of feedforward layer.\n",
    "            num_heads: number of attention heads.\n",
    "            pos_embed: position embedding layer type.\n",
    "            norm_name: feature normalization type and arguments.\n",
    "            conv_block: bool argument to determine if convolutional block is used.\n",
    "            res_block: bool argument to determine if residual block is used.\n",
    "            dropout_rate: faction of the input units to drop.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # for single channel input 4-channel output with patch size of (96,96,96), feature size of 32 and batch norm\n",
    "            >>> net = UNETR(in_channels=1, out_channels=4, img_size=(96,96,96), feature_size=32, norm_name='batch')\n",
    "\n",
    "            # for 4-channel input 3-channel output with patch size of (128,128,128), conv position embedding and instance norm\n",
    "            >>> net = UNETR(in_channels=4, out_channels=3, img_size=(128,128,128), pos_embed='conv', norm_name='instance')\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if not (0 <= dropout_rate <= 1):\n",
    "            raise AssertionError(\"dropout_rate should be between 0 and 1.\")\n",
    "\n",
    "        if hidden_size % num_heads != 0:\n",
    "            raise AssertionError(\"hidden size should be divisible by num_heads.\")\n",
    "\n",
    "        if pos_embed not in [\"conv\", \"perceptron\"]:\n",
    "            raise KeyError(f\"Position embedding layer of type {pos_embed} is not supported.\")\n",
    "\n",
    "        # ---------------------------\n",
    "        # GLOBAL CONFIG (matches paper figure)\n",
    "        \n",
    "        \n",
    "        # ---------------------------\n",
    "        self.num_layers = 12 # - 12 transformer layers (z1..z12 in the paper)\n",
    "        self.patch_size = (16, 16, 16) # - Patch size 16^3: H×W×D is split into (H/16)×(W/16)×(D/16) tokens\n",
    "        self.feat_size = (\n",
    "            img_size[0] // self.patch_size[0],\n",
    "            img_size[1] // self.patch_size[1],\n",
    "            img_size[2] // self.patch_size[2],\n",
    "        )\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # [Left column in the figure]\n",
    "        # ViT encoder = Patch Embedding + Positional Embedding + 12×(MHSA+MLP)\n",
    "        # Returns x and hidden_states_out:\n",
    "        #   x  -> final transformer output (≈ z12 in the figure)\n",
    "        #   hidden_states_out -> list of intermediate states to tap at z3, z6, z9\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.classification = False\n",
    "        self.vit = ViT(\n",
    "            in_channels=in_channels,\n",
    "            img_size=img_size,\n",
    "            patch_size=self.patch_size,\n",
    "            hidden_size=hidden_size,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            num_heads=num_heads,\n",
    "            pos_embed=pos_embed,\n",
    "            classification=self.classification,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # [Top skip path in the figure]\n",
    "        # A pure CNN block working directly on the input image at full resolution\n",
    "        # (yellow \"Conv 3×3×3 + ReLU (+Norm)\" boxes at H×W×D)\n",
    "        # Provides the highest‑res skip to the decoder.\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.encoder1 = UnetrBasicBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=in_channels,  # = 4 in MRI = channels of input image\n",
    "            out_channels=feature_size,  # = 64 in the image\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Projection‑&‑up blocks fed by ViT taps:\n",
    "        # We first reshape tokens back to a 3D feature map (proj_feat), then\n",
    "        # progressively upsample so their spatial sizes align with the decoder.\n",
    "        #\n",
    "        # enc2  ⇐ z3  (hidden_states_out[3])  → upsample to ~ H/2 × W/2 × D/2\n",
    "        # enc3  ⇐ z6  (hidden_states_out[6])  → upsample to ~ H/4 × W/4 × D/4\n",
    "        # enc4  ⇐ z9  (hidden_states_out[9])  → upsample to ~ H/8 × W/8 × D/8\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.encoder2 = UnetrPrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=hidden_size,          # = 768 = channels of ViT token embeddings\n",
    "            out_channels=feature_size * 2,    # \n",
    "            num_layer=2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder3 = UnetrPrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 4,\n",
    "            num_layer=1,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder4 = UnetrPrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 8,\n",
    "            num_layer=0,  # num_layers represents extra upsample blocks, so 0 means only the initial upsample\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # [Decoder path in the figure]\n",
    "        # Each UnetrUpBlock performs: upsample (\"Deconv 2×2×2\") + concat\n",
    "        # with the matching skip (blue \"C\" circles) + Conv blocks.\n",
    "        #\n",
    "        # decoder5 starts from the deepest transformer output (≈ z12),\n",
    "        # projects it to a feature map and upsamples, then fuses with enc4.\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.decoder5 = UnetrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=hidden_size,          # start from ViT final output (z12)\n",
    "            out_channels=feature_size * 8,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder4 = UnetrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=feature_size * 8,\n",
    "            out_channels=feature_size * 4,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder3 = UnetrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=feature_size * 4,\n",
    "            out_channels=feature_size * 2,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder2 = UnetrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=feature_size * 2,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # [Output head in the figure]\n",
    "        # Final 1×1×1 conv that maps decoder features to the segmentation logits\n",
    "        # (the small gray \"Conv 1×1×1\" block).\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.out = UnetOutBlock(spatial_dims=3, in_channels=feature_size, out_channels=out_channels)  # type: ignore\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Utility: reshape ViT tokens back to 3D feature maps\n",
    "    # Input shape:  (B, N_tokens, hidden_size) with N_tokens = (H/16)*(W/16)*(D/16)\n",
    "    # Output shape: (B, hidden_size, H/16, W/16, D/16) to feed CNN blocks\n",
    "    # -------------------------------------------------------------------------\n",
    "    def proj_feat(self, x, hidden_size, feat_size):\n",
    "        x = x.view(x.size(0), feat_size[0], feat_size[1], feat_size[2], hidden_size)\n",
    "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        return x\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # (Optional) Load pretrained weights for the ViT encoder from a checkpoint.\n",
    "    # Copies patch-embedding, transformer blocks, and final norm parameters.\n",
    "    # -------------------------------------------------------------------------\n",
    "    def load_from(self, weights):\n",
    "        with torch.no_grad():\n",
    "            res_weight = weights\n",
    "            # copy weights from patch embedding\n",
    "            for i in weights[\"state_dict\"]:\n",
    "                print(i)\n",
    "            self.vit.patch_embedding.position_embeddings.copy_(\n",
    "                weights[\"state_dict\"][\"module.transformer.patch_embedding.position_embeddings_3d\"]\n",
    "            )\n",
    "            self.vit.patch_embedding.cls_token.copy_(\n",
    "                weights[\"state_dict\"][\"module.transformer.patch_embedding.cls_token\"]\n",
    "            )\n",
    "            self.vit.patch_embedding.patch_embeddings[1].weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.transformer.patch_embedding.patch_embeddings.1.weight\"]\n",
    "            )\n",
    "            self.vit.patch_embedding.patch_embeddings[1].bias.copy_(\n",
    "                weights[\"state_dict\"][\"module.transformer.patch_embedding.patch_embeddings.1.bias\"]\n",
    "            )\n",
    "\n",
    "            # copy weights from  encoding blocks (default: num of blocks: 12)\n",
    "            for bname, block in self.vit.blocks.named_children():\n",
    "                print(block)\n",
    "                block.loadFrom(weights, n_block=bname)\n",
    "            # last norm layer of transformer\n",
    "            self.vit.norm.weight.copy_(weights[\"state_dict\"][\"module.transformer.norm.weight\"])\n",
    "            self.vit.norm.bias.copy_(weights[\"state_dict\"][\"module.transformer.norm.bias\"])\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # -------------------------------------------------------------\n",
    "        # ViT forward:\n",
    "        #   x  = final transformer output  (≈ z12)\n",
    "        #   hidden_states_out[k] = intermediate tap after block k\n",
    "        # We tap 3, 6, 9 (~ z3, z6, z9 in the paper’s notation).\n",
    "\n",
    "        # Every layer’s output (z3, z6, z9, z12), or (x2, x3, x4, x),\n",
    "        # is still (B, N, 768) (tokens-by-features).\n",
    "\n",
    "        # proj_feat(x3, ...) is computed inline and passed straight into encoder.\n",
    "        # It turns the (B, N, 768) tokens-by-features into (B, C, H/16, W/16, D/16) CNN feature maps.\n",
    "        # -------------------------------------------------------------\n",
    "        x, hidden_states_out = self.vit(x_in)\n",
    "\n",
    "        # Top skip from raw image (full resolution)\n",
    "        enc1 = self.encoder1(x_in)\n",
    "\n",
    "        # Taps from the transformer, reshaped & projected to CNN features\n",
    "        x2 = hidden_states_out[3]   # ~ z3\n",
    "        enc2 = self.encoder2(self.proj_feat(x2, self.hidden_size, self.feat_size))\n",
    "\n",
    "        x3 = hidden_states_out[6]   # ~ z6\n",
    "        enc3 = self.encoder3(self.proj_feat(x3, self.hidden_size, self.feat_size))\n",
    "\n",
    "        x4 = hidden_states_out[9]   # ~ z9\n",
    "        enc4 = self.encoder4(self.proj_feat(x4, self.hidden_size, self.feat_size))\n",
    "\n",
    "        # Bridge from the deepest token features (≈ z12) to decoder feature map\n",
    "        dec4 = self.proj_feat(x, self.hidden_size, self.feat_size)\n",
    "\n",
    "        # Decoder with skip concatenations (the \"C\" nodes in the figure)\n",
    "        dec3 = self.decoder5(dec4, enc4)  # fuse with enc4 (H/8 scale)\n",
    "        dec2 = self.decoder4(dec3, enc3)  # fuse with enc3 (H/4 scale)\n",
    "        dec1 = self.decoder3(dec2, enc2)  # fuse with enc2 (H/2 scale)\n",
    "        out = self.decoder2(dec1, enc1)   # fuse with enc1 (H scale)\n",
    "\n",
    "        # Final prediction\n",
    "        logits = self.out(out)\n",
    "        return logits\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
